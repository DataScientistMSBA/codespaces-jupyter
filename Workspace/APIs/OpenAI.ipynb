{"cells":[{"cell_type":"markdown","metadata":{"id":"934YVtPDg0-4"},"source":["**Install packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24401,"status":"ok","timestamp":1670449035392,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"6iC28l3Tg1Zr","outputId":"7af5aeb7-4d3d-47de-8235-b6aaca667892"},"outputs":[],"source":["!pip install openai"]},{"cell_type":"markdown","metadata":{"id":"s86x61Ewgwxa"},"source":["**Import packages**"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1670449035392,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"sUl4_rhIUUel"},"outputs":[],"source":["import os\n","import openai\n","# from google.colab import drive\n","import json"]},{"cell_type":"markdown","metadata":{"id":"jaH9Mh2jg_2m"},"source":["**Lookup and Assign API Key**"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"executionInfo":{"elapsed":5004,"status":"error","timestamp":1670448996589,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"aI5Cn3fWcMog","outputId":"13a7a957-2636-4de8-c825-530faf4bb2bc"},"outputs":[{"ename":"MessageError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-8d06f7531935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    122\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    125\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["# drive.mount('/content/gdrive/') # can use sidebar instead"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":665,"status":"ok","timestamp":1670449075570,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"Gt01QDAbcMlq","outputId":"5005e652-12b4-49f9-a90c-b6e8d6516128"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Security\n"]}],"source":["# cd gdrive/MyDrive/'Colab Notebooks'/Security/\n","cd drive/MyDrive/'Colab Notebooks'/Security/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_vp9p_uhOD3"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":639,"status":"ok","timestamp":1670449106456,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"D8gTtRwUg_WY","outputId":"c7ea6cbc-afef-4512-e462-8c7f9edef0bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["API key loaded\n"]}],"source":["fileObject = open(\"/workspaces/codespaces-jupyter/Workspace/Security/Passwords.txt\", \"r\", encoding='utf-8-sig')\n","data = fileObject.read()\n","data = json.loads(data)\n","openai.api_key = data['OpenAI']['key']\n","print('API key loaded')"]},{"cell_type":"markdown","metadata":{"id":"hufXUSD0glnX"},"source":["[**Q&A**](https://beta.openai.com/examples/default-qa)\n","\n","*Answer questions based on existing knowledge.*"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":8118,"status":"ok","timestamp":1670449156212,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"KQFKWdlFgj62","outputId":"f79f34e4-181c-4121-e1f5-12d224db37be"},"outputs":[{"ename":"RateLimitError","evalue":"You exceeded your current quota, please check your plan and billing details.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      2\u001b[0m   model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-002\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m   prompt\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mI am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39mUnknown\u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mQ: \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mstr\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mAsk a question: \u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m}\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mA:\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m   temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m   max_tokens\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m   top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m   frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m   presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m   stop\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     10\u001b[0m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m     )\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    681\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    684\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n","\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."]}],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=f\"I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \\\"Unknown\\\".\\n\\nQ: {str(input('Ask a question: '))}\\nA:\",\n","  temperature=0,\n","  max_tokens=100,\n","  top_p=1,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"\\n\"])\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"IeGbJTxVTiNQ"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"HG-6PFjZTi0_"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"5g4DrUz3ntnb"},"source":["[**Grammar correction**](https://beta.openai.com/examples/default-grammar)\n","\n","*Corrects sentences into standard English.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1654629835945,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"mhMD3GPrgj4c","outputId":"f314b661-d359-4ca2-dfaa-6f4befb88c0e"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"She didn't go to the market.\""]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["# response = openai.Completion.create(\n","#   model=\"text-davinci-002\",\n","#   prompt=\"Correct this to standard English:\\n\\nShe no went to the market.\",\n","#   temperature=0,\n","#   max_tokens=60,\n","#   top_p=1.0,\n","#   frequency_penalty=0.0,\n","#   presence_penalty=0.0)\n","response['choices'][0]['text'].replace('\\n','')"]},{"cell_type":"markdown","metadata":{"id":"WSc9p8nNxhHo"},"source":["[**Summarize for a 2nd grader**](https://beta.openai.com/examples/default-summarize)\n","\n","*Translates difficult text into simpler concepts.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBRAvLChgjze"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Summarize this for a second-grade student:\\n\\nJupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\",\n","  temperature=0.7,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"8c_DPNzOxpG9"},"source":["[**Natural language to OpenAI API**](https://beta.openai.com/examples/default-openai-api)\n","\n","*Create code to call to the OpenAI API using a natural language instruction.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":1567,"status":"error","timestamp":1654636961673,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"XWKM1qligjxL","outputId":"a6c473aa-629b-4ab8-f694-57fa81708a7c"},"outputs":[{"ename":"InvalidRequestError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-74-47e33c393cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   stop=[\"\\\"\\\"\\\"\"])\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         )\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    328\u001b[0m             return (\n\u001b[1;32m    329\u001b[0m                 self._interpret_response_line(\n\u001b[0;32m--> 330\u001b[0;31m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m                 ),\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             raise self.handle_error_response(\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             )\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidRequestError\u001b[0m: No such model: code-davinci-002"]}],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"\\\"\\\"\\\"\\nUtil exposes the following:\\nutil.openai() -> authenticates & returns the openai module, which has the following functions:\\nopenai.Completion.create(\\n    prompt=\\\"<my prompt>\\\", # The prompt to start completing from\\n    max_tokens=123, # The max number of tokens to generate\\n    temperature=1.0 # A measure of randomness\\n    echo=True, # Whether to return the prompt in addition to the generated completion\\n)\\n\\\"\\\"\\\"\\nimport util\\n\\\"\\\"\\\"\\nCreate an OpenAI completion starting from the prompt \\\"Once upon an AI\\\", no more than 5 tokens. Does not include the prompt.\\n\\\"\\\"\\\"\\n\",\n","  temperature=0,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"\\\"\\\"\\\"\"])\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"e214Cot1xyzv"},"source":["[**Text to command**](https://beta.openai.com/examples/default-text-to-command)\n","\n","*Translate text into programmatic commands.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qn9wqDHDgjvL"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Convert this text to a programmatic command:\\n\\nExample: Ask Constance if we need some bread\\nOutput: send-msg `find constance` Do we need some bread?\\n\\nContact the ski store and figure out if I can get my skis fixed before I leave on Thursday\",\n","  temperature=0,\n","  max_tokens=100,\n","  top_p=1.0,\n","  frequency_penalty=0.2,\n","  presence_penalty=0.0,\n","  stop=[\"\\n\"])\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"ULVJeKRbx4Yz"},"source":["[**English to other languages**](https://beta.openai.com/examples/default-translate)\n","\n","*Translates English text into French, Spanish and Japanese.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Szq800Oygjsz"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Translate this into 1. French, 2. Spanish and 3. Japanese:\\n\\nWhat rooms do you have available?\\n\\n1.\",\n","  temperature=0.3,\n","  max_tokens=100,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"GjoGCp7zx42O"},"source":["[**Natural language to Stripe API**](https://beta.openai.com/examples/default-stripe-api)\n","\n","*Create code to call the Stripe API using natural language.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":662,"status":"error","timestamp":1654636974525,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"Oewepn2hxik1","outputId":"91dff237-1348-4f0d-f2f5-69de1ff4138a"},"outputs":[{"ename":"InvalidRequestError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-75-44dd5c7e8356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   stop=[\"\\\"\\\"\\\"\"])\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         )\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    328\u001b[0m             return (\n\u001b[1;32m    329\u001b[0m                 self._interpret_response_line(\n\u001b[0;32m--> 330\u001b[0;31m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m                 ),\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             raise self.handle_error_response(\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             )\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidRequestError\u001b[0m: No such model: code-davinci-002"]}],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"\\\"\\\"\\\"\\nUtil exposes the following:\\n\\nutil.stripe() -> authenticates & returns the stripe module; usable as stripe.Charge.create etc\\n\\\"\\\"\\\"\\nimport util\\n\\\"\\\"\\\"\\nCreate a Stripe token using the users credit card: 5555-4444-3333-2222, expiration date 12 / 28, cvc 521\\n\\\"\\\"\\\"\",\n","  temperature=0,\n","  max_tokens=100,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"\\\"\\\"\\\"\"])\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"1CfQQqJoyHzK"},"source":["[**SQL translate**](https://beta.openai.com/examples/default-sql-translate)\n","\n","*Translate natural language to SQL queries.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGqfXe3dxjCP"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"### Postgres SQL tables, with their properties:\\n#\\n# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n#\\n### A query to list the names of the departments which employed more than 10 employees in the last 3 months\\nSELECT\",\n","  temperature=0,\n","  max_tokens=150,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"#\", \";\"])\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"ZoNQyZ33yOKn"},"source":["[**Parse unstructured data**](https://beta.openai.com/examples/default-parse-data)\n","\n","*Create tables from long form text by specifying a structure and supplying some examples.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuGTj_eUxi_8"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"A table summarizing the fruits from Goocrux:\\n\\nThere are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\\n\\n| Fruit | Color | Flavor |\",\n","  temperature=0,\n","  max_tokens=100,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"BRSDF9-zyVYM"},"source":["[**Classification**](https://beta.openai.com/examples/default-classification)\n","\n","*Classify items into categories via example.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":2291,"status":"ok","timestamp":1654636983696,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"akHYGAZvxi94","outputId":"a0dc2a61-7c56-403c-8540-3e7109a75c0b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' TechnologyFacebookCategory: Social MediaFedexCategory: Delivery'"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"The following is a list of companies and the categories they fall into:\\n\\nApple, Facebook, Fedex\\n\\nApple\\nCategory:\",\n","  temperature=0,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response['choices'][0]['text'].replace('\\n','')"]},{"cell_type":"markdown","metadata":{"id":"Qlk3htCOy2h5"},"source":["[**Python to natural language**](https://beta.openai.com/examples/default-python-to-natural-language)\n","\n","*Explain a piece of Python code in human understandable language.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fd_4AzERxi5e"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"# Python 3 \\ndef remove_common_prefix(x, prefix, ws_prefix): \\n    x[\\\"completion\\\"] = x[\\\"completion\\\"].str[len(prefix) :] \\n    if ws_prefix: \\n        # keep the single whitespace as prefix \\n        x[\\\"completion\\\"] = \\\" \\\" + x[\\\"completion\\\"] \\nreturn x \\n\\n# Explanation of what the code does\\n\\n#\",\n","  temperature=0,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"#\"])\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"XQ3VZY8KzJPf"},"source":["[**Movie to Emoji**](https://beta.openai.com/examples/default-movie-to-emoji)\n","\n","*Convert movie titles into emoji.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_s1LxuUxi3N"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Convert movie titles into emoji.\\n\\nBack to the Future: 👨👴🚗🕒 \\nBatman: 🤵🦇 \\nTransformers: 🚗🤖 \\nStar Wars:\",\n","  temperature=0.8,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"\\n\"])\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"j22OZBCUzYgn"},"source":["[**Calculate Time Complexity**](https://beta.openai.com/examples/default-time-complexity)\n","\n","*Find the time complexity of a function.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXk-WYfIxi0w"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"def foo(n, k):\\naccum = 0\\nfor i in range(n):\\n    for l in range(k):\\n        accum += i\\nreturn accum\\n\\\"\\\"\\\"\\nThe time complexity of this function is\",\n","  temperature=0,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"\\n\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"scyCV_NHzhbx"},"source":["[**Translate programming languages**](https://beta.openai.com/examples/default-translate-code)\n","\n","*To translate from one programming language to another we can use the comments to specify the source and target languages.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTXmA7QvzYIN"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"##### Translate this function  from Python into Haskell\\n### Python\\n    \\n    def predict_proba(X: Iterable[str]):\\n        return np.array([predict_one_probas(tweet) for tweet in X])\\n    \\n### Haskell\",\n","  temperature=0,\n","  max_tokens=54,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"###\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"fwW-vSEWzrwo"},"source":["[**Advanced tweet classifier**](https://beta.openai.com/examples/default-adv-tweet-classifier)\n","\n","*This is an advanced prompt for detecting sentiment. It allows you to provide it with a list of status updates and then provide a sentiment for each one.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":145,"status":"ok","timestamp":1654637022776,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"PUpnzoxXzudA","outputId":"381eaef1-5d8e-4625-b1f3-75cbbce2bf8c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n1. Negative\\n2. Negative\\n3. Positive\\n4. Positive\\n5. Negative'"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Classify the sentiment in these tweets:\\n\\n1. \\\"I can't stand homework\\\"\\n2. \\\"This sucks. I'm bored 😠\\\"\\n3. \\\"I can't wait for Halloween!!!\\\"\\n4. \\\"My cat is adorable ❤️❤️\\\"\\n5. \\\"I hate chocolate\\\"\\n\\nTweet sentiment ratings:\",\n","  temperature=0,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response['choices'][0]['text']"]},{"cell_type":"markdown","metadata":{"id":"FK9Tr2A2zq-M"},"source":["[**Explain code**](https://beta.openai.com/examples/default-explain-code)\n","\n","*Explain a complicated piece of code.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_VW7fbvzYF_"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"class Log:\\n    def __init__(self, path):\\n        dirname = os.path.dirname(path)\\n        os.makedirs(dirname, exist_ok=True)\\n        f = open(path, \\\"a+\\\")\\n\\n        # Check that the file is newline-terminated\\n        size = os.path.getsize(path)\\n        if size > 0:\\n            f.seek(size - 1)\\n            end = f.read(1)\\n            if end != \\\"\\\\n\\\":\\n                f.write(\\\"\\\\n\\\")\\n        self.f = f\\n        self.path = path\\n\\n    def log(self, event):\\n        event[\\\"_event_id\\\"] = str(uuid.uuid4())\\n        json.dump(event, self.f)\\n        self.f.write(\\\"\\\\n\\\")\\n\\n    def state(self):\\n        state = {\\\"complete\\\": set(), \\\"last\\\": None}\\n        for line in open(self.path):\\n            event = json.loads(line)\\n            if event[\\\"type\\\"] == \\\"submit\\\" and event[\\\"success\\\"]:\\n                state[\\\"complete\\\"].add(event[\\\"id\\\"])\\n                state[\\\"last\\\"] = event\\n        return state\\n\\n\\\"\\\"\\\"\\nHere's what the above class is doing:\\n1.\",\n","  temperature=0,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"\\\"\\\"\\\"\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"2QoeOVZHz9PY"},"source":["[**Keywords**](https://beta.openai.com/examples/default-keywords)\n","\n","*Extract keywords from a block of text. At a lower temperature it picks keywords from the text. At a higher temperature it will generate related keywords which can be helpful for creating search indexes.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2663,"status":"ok","timestamp":1654637083127,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"3JDCCfYyzYCq","outputId":"36cfcb7b-9c59-4b84-dc11-b8c3810beedb"},"outputs":[{"data":{"text/plain":["<OpenAIObject text_completion id=cmpl-5GaymnLgWhlsABPEkDSNuzPat8juw at 0x7f52710c83b0> JSON: {\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"length\",\n","      \"index\": 0,\n","      \"logprobs\": null,\n","      \"text\": \"\\n\\n-Black-on-black ware \\n-Puebloan Native American ceramic artists \\n-Northern New Mexico \\n-Reduction-fired blackware \\n-Pueblo artists \\n-Smooth surface \\n-Burnishing \\n-Refractory slip \"\n","    }\n","  ],\n","  \"created\": 1654637080,\n","  \"id\": \"cmpl-5GaymnLgWhlsABPEkDSNuzPat8juw\",\n","  \"model\": \"text-davinci-002\",\n","  \"object\": \"text_completion\"\n","}"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Extract keywords from this text:\\n\\nBlack-on-black ware is a 20th- and 21st-century pottery tradition developed by the Puebloan Native American ceramic artists in Northern New Mexico. Traditional reduction-fired blackware has been made for centuries by pueblo artists. Black-on-black ware of the past century is produced with a smooth surface, with the designs applied through selective burnishing or the application of refractory slip. Another style involves carving or incising designs and selectively polishing the raised areas. For generations several families from Kha'po Owingeh and P'ohwhóge Owingeh pueblos have been making black-on-black ware with the techniques passed down from matriarch potters. Artists from other pueblos have also produced black-on-black ware. Several contemporary artists have created works honoring the pottery of their ancestors.\",\n","  temperature=0.3,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.8,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"t9z7xPYR0IJ-"},"source":["[**Factual answering**](https://beta.openai.com/examples/default-factual-answering)\n","\n","*Guide the model towards factual answering by showing it how to respond to questions that fall outside its knowledge base. Using a '?' to indicate a response to words and phrases that it doesn't know provides a natural response that seems to work better than more abstract replies.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ahPeRZbzX-k"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Q: Who is Batman?\\nA: Batman is a fictional comic book character.\\n\\nQ: What is torsalplexity?\\nA: ?\\n\\nQ: What is Devz9?\\nA: ?\\n\\nQ: Who is George Lucas?\\nA: George Lucas is American film director and producer famous for creating Star Wars.\\n\\nQ: What is the capital of California?\\nA: Sacramento.\\n\\nQ: What orbits the Earth?\\nA: The Moon.\\n\\nQ: Who is Fred Rickerson?\\nA: ?\\n\\nQ: What is an atom?\\nA: An atom is a tiny particle that makes up everything.\\n\\nQ: Who is Alvan Muntz?\\nA: ?\\n\\nQ: What is Kozar-09?\\nA: ?\\n\\nQ: How many moons does Mars have?\\nA: Two, Phobos and Deimos.\\n\\nQ: What's a language model?\\nA:\",\n","  temperature=0,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"FV7EnJCA0-TR"},"source":["[**Ad from product description**](https://beta.openai.com/examples/default-ad-product-description)\n","\n","*Turn a product description into ad copy.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qC7KJCai0-IT"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Write a creative ad for the following product to run on Facebook aimed at parents:\\n\\nProduct: Learning Room is a virtual environment to help students from kindergarten to high school excel in school.\",\n","  temperature=0.5,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"TdC1Ld3M0-AG"},"source":["[**Product name generator**](https://beta.openai.com/examples/default-product-name-gen)\n","\n","*Create product names from examples words. Influenced by a community prompt.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekRk63UP0942"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\",\n","  temperature=0.8,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"FrJhWUBY09g9"},"source":["[**TL;DR summarization**](https://beta.openai.com/examples/default-tldr-summary)\n","\n","*Summarize text by adding a 'tl;dr:' to the end of a text passage. It shows that the API understands how to perform a number of tasks with no instructions.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3864,"status":"ok","timestamp":1654637101004,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"-lugku2n09Z1","outputId":"c58c9327-290b-49d2-8842-a0f4ec8ad431"},"outputs":[{"data":{"text/plain":["<OpenAIObject text_completion id=cmpl-5Gaz345n746DY0I5Z6vVDGWWkxwIP at 0x7f52710c8350> JSON: {\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"logprobs\": null,\n","      \"text\": \"\\n\\nA neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more if the star was especially metal-rich.\"\n","    }\n","  ],\n","  \"created\": 1654637097,\n","  \"id\": \"cmpl-5Gaz345n746DY0I5Z6vVDGWWkxwIP\",\n","  \"model\": \"text-davinci-002\",\n","  \"object\": \"text_completion\"\n","}"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more if the star was especially metal-rich.[1] Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical white holes, quark stars, and strange stars.[2] Neutron stars have a radius on the order of 10 kilometres (6.2 mi) and a mass of about 1.4 solar masses.[3] They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core past white dwarf star density to that of atomic nuclei.\\n\\nTl;dr\",\n","  temperature=0.7,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"nU-Q45fB09Qs"},"source":["[**Python bug fixer**](https://beta.openai.com/examples/default-fix-python-bugs)\n","\n","*There's a number of ways of structuring the prompt for checking for bugs. Here we add a comment suggesting that source code is buggy, and then ask codex to generate a fixed code.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXvTtPcC08-k"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"##### Fix bugs in the below function\\n \\n### Buggy Python\\nimport Random\\na = random.randint(1,12)\\nb = random.randint(1,12)\\nfor i in range(10):\\n    question = \\\"What is \\\"+a+\\\" x \\\"+b+\\\"? \\\"\\n    answer = input(question)\\n    if answer = a*b\\n        print (Well done!)\\n    else:\\n        print(\\\"No.\\\")\\n    \\n### Fixed Python\",\n","  temperature=0,\n","  max_tokens=182,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"###\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"sfR207YY080y"},"source":["[**Spreadsheet creator**](https://beta.openai.com/examples/default-spreadsheet-gen)\n","\n","*Create spreadsheets of various kinds of data. It's a long prompt but very versatile. Output can be copy+pasted into a text file and saved as a .csv with pipe separators.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNYSHyj808tp"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"A two-column spreadsheet of top science fiction movies and the year of release:\\n\\nTitle|  Year of release\",\n","  temperature=0.5,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"b20_iVJR08iq"},"source":["[**JavaScript helper chatbot**](https://beta.openai.com/examples/default-js-helper)\n","\n","*This is a message-style chatbot that can answer questions about using JavaScript. It uses a few examples to get the conversation started.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogcIkq7A1s8C"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"You: How do I combine arrays?\\nJavaScript chatbot: You can use the concat() method.\\nYou: How do you make an alert appear after 10 seconds?\\nJavaScript chatbot\",\n","  temperature=0,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.5,\n","  presence_penalty=0.0,\n","  stop=[\"You:\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"nQFtURxh08RA"},"source":["[**ML/AI language model tutor**](https://beta.openai.com/examples/default-ml-ai-tutor)\n","\n","*This is a QA-style chatbot that answers questions about language models.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsKkijhE08Jy"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"ML Tutor: I am a ML/AI language model tutor\\nYou: What is a language model?\\nML Tutor: A language model is a statistical model that describes the probability of a word given the previous words.\\nYou: What is a statistical model?\",\n","  temperature=0.3,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.5,\n","  presence_penalty=0.0,\n","  stop=[\"You:\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"Uie1Xcnf08B6"},"source":["[**Science fiction book list maker**](https://beta.openai.com/examples/default-sci-fi-book-list)\n","\n","*This makes a list of science fiction books and stops when it reaches #10.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQpZyc1u076R"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"List 10 science fiction books:\",\n","  temperature=0.5,\n","  max_tokens=200,\n","  top_p=1.0,\n","  frequency_penalty=0.52,\n","  presence_penalty=0.5,\n","  stop=[\"11.\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"RNR_Sosw07x9"},"source":["[**Tweet classifier**](https://beta.openai.com/examples/default-tweet-classifier)\n","\n","*This is a basic prompt for detecting sentiment.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1614,"status":"ok","timestamp":1654637134709,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"ckjjTO7c07qj","outputId":"a29ea15b-74fb-400c-d34e-86a94072c37f"},"outputs":[{"data":{"text/plain":["<OpenAIObject text_completion id=cmpl-5Gazc2DFuewZ6ptJJxAXQoIx8NqOz at 0x7f52710cd2f0> JSON: {\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"logprobs\": null,\n","      \"text\": \" Positive\"\n","    }\n","  ],\n","  \"created\": 1654637132,\n","  \"id\": \"cmpl-5Gazc2DFuewZ6ptJJxAXQoIx8NqOz\",\n","  \"model\": \"text-davinci-002\",\n","  \"object\": \"text_completion\"\n","}"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\\n\\nTweet: \\\"I loved the new Batman movie!\\\"\\nSentiment:\",\n","  temperature=0,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.5,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"LR1ayQcG07gk"},"source":["[**Airport code extractor**](https://beta.openai.com/examples/default-airport-codes)\n","\n","*A simple prompt for extracting airport codes from text.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHm5rSo607Z7"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Extract the airport codes from this text:\\n\\nText: \\\"I want to fly from Los Angeles to Miami.\\\"\\nAirport codes: LAX, MIA\\n\\nText: \\\"I want to fly from Orlando to Boston\\\"\\nAirport codes:\",\n","  temperature=0,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"\\n\"])\\\n","response"]},{"cell_type":"markdown","metadata":{"id":"L6fWV2d907Qz"},"source":["[**SQL request**](https://beta.openai.com/examples/default-sql-request)\n","\n","*Create simple SQL queries.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nxavbek07Jt"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Create a SQL request to find all users who live in California and have over 1000 credits:\",\n","  temperature=0.3,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"Q8gtvwV_07BW"},"source":["[**Extract contact information**](https://beta.openai.com/examples/default-extract-contact-info)\n","\n","*Extract contact information from a block of text.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ACHeLLG05s7"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Extract the name and mailing address from this email:\\n\\nDear Kelly,\\n\\nIt was great to talk to you at the seminar. I thought Jane's talk was quite good.\\n\\nThank you for the book. Here's my address 2111 Ash Lane, Crestview CA 92002\\n\\nBest,\\n\\nMaya\\n\\nName:\",\n","  temperature=0,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"Igs5Z7Gh05hg"},"source":["[**JavaScript to Python**](https://beta.openai.com/examples/default-js-to-py)\n","\n","*Convert simple JavaScript expressions into Python.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfcl8Mep05ZZ"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"#JavaScript to Python:\\nJavaScript: \\ndogs = [\\\"bill\\\", \\\"joe\\\", \\\"carl\\\"]\\ncar = []\\ndogs.forEach((dog) {\\n    car.push(dog);\\n});\\n\\nPython:\",\n","  temperature=0,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"rumyMsSQ05Oe"},"source":["[**Friend chat**](https://beta.openai.com/examples/default-friend-chat)\n","\n","*Emulate a text message conversation.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jfxXZZ506Xf"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"You: What have you been up to?\\nFriend: Watching old movies.\\nYou: Did you watch anything interesting?\\nFriend:\",\n","  temperature=0.5,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.5,\n","  presence_penalty=0.0,\n","  stop=[\"You:\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"0WTaqfDW05Hr"},"source":["[**Mood to color**](https://beta.openai.com/examples/default-mood-color)\n","\n","*Turn a text description into a color.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVm1AzBI05Ai"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"The CSS code for a color like a blue sky at dusk:\\n\\nbackground-color: #\",\n","  temperature=0,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\";\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"Pv0fYr40044b"},"source":["[**Write a Python docstring**](https://beta.openai.com/examples/default-python-docstring)\n","\n","*An example of how to create a docstring for a given Python function. We specify the Python version, paste in the code, and then ask within a comment for a docstring, and give a characteristic beginning of a docstring (\"\"\").*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4szVY1C04uT"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"# Python 3.7\\n \\ndef randomly_split_dataset(folder, filename, split_ratio=[0.8, 0.2]):\\n    df = pd.read_json(folder + filename, lines=True)\\n    train_name, test_name = \\\"train.jsonl\\\", \\\"test.jsonl\\\"\\n    df_train, df_test = train_test_split(df, test_size=split_ratio[1], random_state=42)\\n    df_train.to_json(folder + train_name, orient='records', lines=True)\\n    df_test.to_json(folder + test_name, orient='records', lines=True)\\nrandomly_split_dataset('finetune_data/', 'dataset.jsonl')\\n    \\n# An elaborate, high quality docstring for the above function:\\n\\\"\\\"\\\"\",\n","  temperature=0,\n","  max_tokens=150,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"#\", \"\\\"\\\"\\\"\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"jDZLvGeH04mo"},"source":["[**Analogy maker**](https://beta.openai.com/examples/default-analogy-maker)\n","\n","*Create analogies. Modified from a community prompt to require fewer examples.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdKX_ga104f9"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Create an analogy for this phrase:\\n\\nQuestions are arrows in that:\",\n","  temperature=0.5,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"GHaPomiw04W6"},"source":["[**JavaScript one line function**](https://beta.openai.com/examples/default-js-one-line)\n","\n","*Turn a JavaScript function into a one liner.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9o8cdVlt04P_"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"code-davinci-002\",\n","  prompt=\"Use list comprehension to convert this into one line of JavaScript:\\n\\ndogs.forEach((dog) => {\\n    car.push(dog);\\n});\\n\\nJavaScript one line version:\",\n","  temperature=0,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\";\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"9DbDqbhQ04Hr"},"source":["[**Micro horror story creator**](https://beta.openai.com/examples/default-micro-horror)\n","\n","*Creates two to three sentence short horror stories from a topic input.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8lwBmnh04Ap"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Topic: Breakfast\\nTwo-Sentence Horror Story: He always stops crying when I pour the milk on his cereal. I just have to remember not to let him see his face on the carton.\\n    \\nTopic: Wind\\nTwo-Sentence Horror Story:\",\n","  temperature=0.8,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.5,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"JglOGJCH035Y"},"source":["[**Third-person converter**](https://beta.openai.com/examples/default-third-person)\n","\n","*Converts first-person POV to the third-person. This is modified from a community prompt to use fewer examples.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EM6iMsg_03x0"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Convert this from first-person to third person (gender female):\\n\\nI decided to make a movie about Ada Lovelace.\",\n","  temperature=0,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"zMlcWF-N03qD"},"source":["[**Notes to summary**](https://beta.openai.com/examples/default-notes-summary)\n","\n","*Turn meeting notes into a summary.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2699,"status":"ok","timestamp":1654637158907,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"ngctDPIc03ie","outputId":"6eec3417-e744-43cc-a327-c201da5e2393"},"outputs":[{"data":{"text/plain":["<OpenAIObject text_completion id=cmpl-5Gb00IxuB1QWcwaJYseEJR7alAcs0 at 0x7f52710cd350> JSON: {\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"logprobs\": null,\n","      \"text\": \"\\n\\nTom said that profits were up 50%. Jane said that new servers were online. Kjel said that they needed more time to fix the software. Jane said that she was happy to help. Parkman said that beta testing was almost done.\"\n","    }\n","  ],\n","  \"created\": 1654637156,\n","  \"id\": \"cmpl-5Gb00IxuB1QWcwaJYseEJR7alAcs0\",\n","  \"model\": \"text-davinci-002\",\n","  \"object\": \"text_completion\"\n","}"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Convert my short hand into a first-hand account of the meeting:\\n\\nTom: Profits up 50%\\nJane: New servers are online\\nKjel: Need more time to fix software\\nJane: Happy to help\\nParkman: Beta testing almost done\",\n","  temperature=0,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"JcELT_Hn03a7"},"source":["[**VR fitness idea generator**](https://beta.openai.com/examples/default-vr-fitness)\n","\n","*Create ideas for fitness and virtual reality games.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRSFXNbc03UD"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Brainstorm some ideas combining VR and fitness:\",\n","  temperature=0.6,\n","  max_tokens=150,\n","  top_p=1.0,\n","  frequency_penalty=1,\n","  presence_penalty=1)\n","response"]},{"cell_type":"markdown","metadata":{"id":"uQIMGXqU03LA"},"source":["[**ESRB rating**](https://beta.openai.com/examples/default-esrb-rating)\n","\n","*Categorize text based upon ESRB ratings.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOpKVExs03EU"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Provide an ESRB rating for the following text:\\n\\n\\\"i'm going to blow your brains out with my ray gun then stomp on your guts.\\\"\\n\\nESRB rating:\",\n","  temperature=0.3,\n","  max_tokens=60,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0,\n","  stop=[\"\\n\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"-5J__HtJ0265"},"source":["[**Essay outline**](https://beta.openai.com/examples/default-essay-outline)\n","\n","*Create an outline for an essay about Nikola Tesla and his contributions to technology:*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5246,"status":"ok","timestamp":1654637185486,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"tsqoIR_w02yz","outputId":"60915b40-1e9a-4192-e735-717dfc1923ba"},"outputs":[{"data":{"text/plain":["<OpenAIObject text_completion id=cmpl-5Gb0ON8pCdGSfWw9XyrO2EaoVU9nn at 0x7f52710d6c50> JSON: {\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"logprobs\": null,\n","      \"text\": \"\\n\\n1. Introduce Nikola Tesla and his work in the field of electricity and magnetism.\\n\\n2. Discuss Tesla's contributions to the development of alternating current (AC) electricity.\\n\\n3. Describe Tesla's development of the Tesla coil and its impact on wireless technology.\\n\\n4. Discuss Tesla's experiments with X-rays and their implications for the future of medical technology.\\n\\n5. Conclude with a discussion of Tesla's legacy and his impact on the development of modern technology.\"\n","    }\n","  ],\n","  \"created\": 1654637180,\n","  \"id\": \"cmpl-5Gb0ON8pCdGSfWw9XyrO2EaoVU9nn\",\n","  \"model\": \"text-davinci-002\",\n","  \"object\": \"text_completion\"\n","}"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Create an outline for an essay about Nikola Tesla and his contributions to technology:\",\n","  temperature=0,\n","  max_tokens=150,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"wC8MsJaZ02qT"},"source":["[**Recipe creator (eat at your own risk)**](https://beta.openai.com/examples/default-recipe-generator)\n","\n","*Create a recipe from a list of ingredients.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pj-gukdp02jX"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Write a recipe based on these ingredients and instructions:\\n\\nFrito Pie\\n\\nIngredients:\\nFritos\\nChili\\nShredded cheddar cheese\\nSweet white or red onions, diced small\\nSour cream\\n\\nInstructions:\",\n","  temperature=0.3,\n","  max_tokens=120,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"j-TonG3702c5"},"source":["[**Chat**](https://beta.openai.com/examples/default-chat)\n","\n","*Open ended conversation with an AI assistant.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQJp1DUe02Vs"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\\n\\nHuman: Hello, who are you?\\nAI: I am an AI created by OpenAI. How can I help you today?\\nHuman: I'd like to cancel my subscription.\\nAI:\",\n","  temperature=0.9,\n","  max_tokens=150,\n","  top_p=1,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.6,\n","  stop=[\" Human:\", \" AI:\"])\n","response"]},{"cell_type":"markdown","metadata":{"id":"LD86rsro02Ox"},"source":["[**Marv the sarcastic chat bot**](https://beta.openai.com/examples/default-marv-sarcastic-chat)\n","\n","*Marv is a factual chatbot that is also sarcastic.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Md3_0mBG02IV"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Marv is a chatbot that reluctantly answers questions with sarcastic responses:\\n\\nYou: How many pounds are in a kilogram?\\nMarv: This again? There are 2.2 pounds in a kilogram. Please make a note of this.\\nYou: What does HTML stand for?\\nMarv: Was Google too busy? Hypertext Markup Language. The T is for try to ask better questions in the future.\\nYou: When did the first airplane fly?\\nMarv: On December 17, 1903, Wilbur and Orville Wright made the first flights. I wish they’d come and take me away.\\nYou: What is the meaning of life?\\nMarv: I’m not sure. I’ll ask my friend Google.\\nYou: What time is it?\\nMarv:\",\n","  temperature=0.5,\n","  max_tokens=60,\n","  top_p=0.3,\n","  frequency_penalty=0.5,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"Q6nrq0Kh02BQ"},"source":["[**Turn by turn directions**](https://beta.openai.com/examples/default-turn-by-turn-directions)\n","\n","*Convert natural language to turn-by-turn directions.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wI3KjrhM015u"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Create a numbered list of turn-by-turn directions from this text: \\n\\nGo south on 95 until you hit Sunrise boulevard then take it east to us 1 and head south. Tom Jenkins bbq will be on the left after several miles.\",\n","  temperature=0.3,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"bOCx8KzJ01x8"},"source":["[**Restaurant review creator**](https://beta.openai.com/examples/default-restaurant-review)\n","\n","*Turn a few words into a restaurant review.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"doI4Qp8601bw"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Write a restaurant review based on these notes:\\n\\nName: The Blue Wharf\\nLobster great, noisy, service polite, prices good.\\n\\nReview:\",\n","  temperature=0.5,\n","  max_tokens=64,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"RW44l16T01Iq"},"source":["[**Create study notes**](https://beta.openai.com/examples/default-study-notes)\n","\n","*Provide a topic and get study notes.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8761,"status":"ok","timestamp":1654637206540,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"N0ssDy7T0vJ0","outputId":"90691b26-7e2a-4040-aa59-2fb549899b31"},"outputs":[{"data":{"text/plain":["<OpenAIObject text_completion id=cmpl-5Gb0fDcnidRdNTldGjpROBCdyrEe9 at 0x7f52710cdd10> JSON: {\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"length\",\n","      \"index\": 0,\n","      \"logprobs\": null,\n","      \"text\": \"\\n\\n1. The Roman Republic was founded in 509 BCE after the city of Rome was sacked by the Gauls. The Roman Republic lasted until the end of the Roman Empire in 476 CE.\\n\\n2. The Roman Republic was a federal state with a complex system of government. The Roman Republic was divided into two classes: the patricians and the plebeians. The patricians were the wealthier class while the plebeians were the poorer class.\\n\\n3. The Roman Republic was ruled by two consuls. The consuls were elected by the people and held office for one year. They were responsible for the administration of justice and the defense of the state.\\n\\n4. The Roman Republic was an olig\"\n","    }\n","  ],\n","  \"created\": 1654637197,\n","  \"id\": \"cmpl-5Gb0fDcnidRdNTldGjpROBCdyrEe9\",\n","  \"model\": \"text-davinci-002\",\n","  \"object\": \"text_completion\"\n","}"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"What are 5 key points I should know when studying Ancient Rome?\",\n","  temperature=0.3,\n","  max_tokens=150,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"markdown","metadata":{"id":"6E7BZXyo0vBI"},"source":["[**Interview questions**](https://beta.openai.com/examples/default-interview-questions)\n","\n","*Create a list of 8 questions for my interview with a science fiction author:*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abGcVry80u18"},"outputs":[],"source":["response = openai.Completion.create(\n","  model=\"text-davinci-002\",\n","  prompt=\"Create a list of 8 questions for my interview with a science fiction author:\",\n","  temperature=0.5,\n","  max_tokens=150,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0)\n","response"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8GYkKYvzX8X"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rqui4xNyzX6E"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2iAxty3zX3s"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBV4nR9IzX1A"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuQjf-zCxiq7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9ykBkbHxinU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":939,"status":"ok","timestamp":1654628204363,"user":{"displayName":"Andrew Hicks","userId":"03768719951752521185"},"user_tz":420},"id":"xALphyhjgjqV","outputId":"0b2193b8-3a4d-4d18-de10-36744fabbd67"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"?Today's date is September 18, 2020.\""]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["response = openai.Completion.create(\n","  engine=\"text-davinci-002\",\n","  prompt=\"What is today's date\",\n","  temperature=0,\n","  max_tokens=150,\n","  top_p=1.0,\n","  frequency_penalty=0.0,\n","  presence_penalty=0.0\n",")\n","new_response = response['choices'][0]['text'].replace('\\n','')\n","new_response"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUOh3aQlgjn5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYyUgnkdgjdk"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPsIfjK8FGlEG7z7f3K0UtW","mount_file_id":"1w5M4DsZf0rKVbpvqXdikFJJENqng7nex","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"}}},"nbformat":4,"nbformat_minor":0}
